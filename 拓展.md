*存储过程与函数的区别：**

存储存储过程是一段代码（过程），存储在数据库中的SQL组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。
函数通常是数据库已定义的方法。存储过程和函数事先进行过编译。存储过程和函数执行不是由程序调用，也不是手动启动。而是由事件触发、激活从而实现执行。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行通过call执行。
存储过程和函数都是属于某个数据库。

1，返回值的区别：函数有一个返回值，而存储过程是通过参数返回的，可以有多个或者没有。

2，调用区别：函数可以在查询语句中直接调用（SELECT调用），而存储过程必须单独调用（ EXECUTE 语句执行）。

3，函数一般情况下是用来计算并返回一个计算结果，而存储过程一般是用来完成特定的数据操作（比如修改插入数据库或执行某些DDL语句等等）。

4，oracle中存储过程和函数都可以返回值，但是函数必须要返回值并且一般返回一个值，而存储过程则没有这个限制。

##### 存储过程：

###### **MySQL创建调用**

**创建：**

```sql
create procedure 存储过程名称（in|out|inout 参数名称 参数类型，……）
begin
过程体;
end
```

1、查询

查询所有存储过程状态

```sql
show procedure status;
```

查看对应数据库下所有存储过程状态

```sql
show procedure status where db="数据库名";
```

查看名称包含Student的存储过程状态

```sql
show procedure status where name like "%Student%";
```

查询存储过程详细代码

```sql
show create procedure 过程名;
```

2、修改

```sql
alter procedure 过程名([过程参数[,…]])过程体；
```

3、删除

```sql
drop procedure 过程名；
```

注：不能在一个存储过程中删除另一个存储过程，只能调用另一个存储过程。

**调用存储过程**

mysql存储过程用call和过程名以及一个括号，括号里面根据需要，加入参数，参数包括输入参数、输出参数、输入输出参数调用。

```sql
call 存储过程名（[过程参数[,...]]）
```

###### **Oracle创建调用**

```plsql
create or replace procedure sample_proc 
as  --声明
 msg varchar2(50);
begin  --执行
  msg:='Hello world';--为参数赋值
  dbms_output.put_line('你好的英文为:'||msg);--输出参数
exception --存贮过程异常
  ;
end;
 
--有三种执行语法
--执行语法1
call sample_proc();
--执行结果
sample_proc ) 成功。
你好的英文为:Hello world
 
--执行语法2
exec sample_proc;
--执行结果
匿名块已完成
你好的英文为:Hello world
 
--执行语法3
set serveroutput on
begin
 sample_proc;
end;
--执行结果
匿名块已完成
你好的英文为:Hello world
```

##### **函数**

###### **MySQL创建**

```plsql
DELIMITER $$ --定义结束符。MySQL默认的结束符是分号，但是函数体中可能用到分号。为								了避免冲突，需要另外定义结束符。

DROP FUNCTION IF EXISTS function_name$$ --如果函数genPerson已经存在了，就删除掉。

CREATE FUNCTION function_name(name varchar(20)) RETURNS varchar(50) --创建函数genPerson，函数的参数是name，返回值是varchar(50)。

BEGIN --函数体放在BEGIN 与 END之间。

  DECLARE str VARCHAR(50) DEFAULT ''; --DECLARE声明变量，str类型是varchar(50)，默认值是空。
  SET @tableName=name;
  SET str=CONCAT('create table ', @tableName,'(id int, name varchar(20));'); 
  --CONCAT连接多个字符串。
  
  return str; --RETURN 返回拼接后的字符串str。
END $$
DELIMITER ;

```

###### Oracle**创建**

```plsql
CREATE [OR REPLACE] FUNCTION function_name
   [ (parameter [,parameter]) ]

   RETURN return_datatype

IS | AS

   [declaration_section]

BEGIN
   executable_section

[EXCEPTION
   exception_section]

END [function_name];
```

**执行：**

```plsql
--调用
select function_name('参数');

--删除
DROP FUNCTION function_name;
```

#### **定时器：**

###### **MySQL创建**

```plsql
CREATE EVENT IF NOT EXISTS 计划名
	-- 计划频率和开启计划时间或者是计划执行的时间
	-- 前一个可以实现持续的计划调度，后一个到指定时间进行调度,执行完结束，没有持续性
	ON SCHEDULE [EVERY 10 SECOND STARTS TIMESTAMP 开启时间] [AT 开启时间]
	-- 当计划执行完成时，是否删除
	ON COMPLETION [NOT] PRESERVE 
	do call 存储过程
```

**查看数据库是否开启调度**

```sql
-- 查看是否开启调度
show variables like '%event_scheduler%';
-- value为OFF，未开启；
-- 开启
SET GLOBAL event_scheduler = 1;
```

**关闭和开启指定定时器**

```sql
ALTER EVENT 定时器名 ON  COMPLETION PRESERVE [ENABLE][DISABLE];
-- 开启ENABLE，关闭DISABLE 
```

**删除定时器**

```sql
drop EVENT 定时器名;
```

###### Oracle创建

```plsql
DECLARE  job_test  number;  -- DECLARE 用来定义unlockTest_timer 的定时器编号
BEGIN
  SYS.DBMS_JOB.SUBMIT(
    job => unlockTest_timer,   --job 指的是定时器编号，在DECLARE 中已经声明
    what => 'pro_test;',       --what 指的是要执行的存储过程，也就是SQL语句
    NEXT_DATE => sysdate,      --next_date 指的是下次执行时间
    INTERVAL => 'sysdate+1/（24*60）'  --interval 指的是每次执行时间的间隔时间   这里是一分钟执行一次
  );
Commit;
End;
```

```plsql
--定时器创建好后，会自动执行。
--查看在执行的定时器，job-定时器编号
SELECT job, next_date, next_sec, failures, broken FROM user_jobs;
```

```plsql
--停止
    begin
       dbms_job.broken(定时器编号,true);
       commit;
    end;

--停止后再启动

   begin
        dbms_job.run(定时器编号）;
        commit;
   end;

--定时器删除
   begin
       dbms_job.remove(定时器编号);
       commit;
    end;
```



#### 触发器：

- **触发器组成**

  1、触发事件
  　　DML或DDL语句。
  2、触发时间
  　　是在触发事件发生之前(before) 还是之后(after) 触发
  3、触发操作
  　　使用PL/SQL块进行相应的数据库操作
  4、触发对象
  　　表、视图、模式、数据库
  5、触发频率
  　　触发器内定义的动作被执行的次数，包括语句级和行级。

语句级触发器；DML操作 insert delete update select
 	行级触发器；
  	系统事件触发器；数据库的关闭启动

用户事件触发器；DDL操作 drop alter create

###### MySQL

MySQL的触发器是按照BEFORE触发器、行操作、AFTER触发器的顺序执行的，其中任何一步发生错误都不会继续执行剩下的操作。如果是对事务表进行的操作，那么会整个作为一个事务被回滚，但是如果是对非事务表进行的操作，那么已经更新的记录将无法回滚，这也是设计触发器的时候需要注意的问题。

```plsql
CREATE

    [DEFINER = { user | CURRENT_USER }]

    TRIGGER trigger_name      --trigger_name：触发器的名称，不能与已经存在的触发器重复；

    trigger_time trigger_event   --trigger_time：{ BEFORE | AFTER }，表示在事件之前或之后触发；trigger_event:：{ INSERT |UPDATE | DELETE }，触发该触发器的具体事件；

    ON tbl_name FOR EACH ROW

    trigger_body   --tbl_name：该触发器作用在tbl_name上；
```

```plsql
SHOW TRIGGERS trigger_name;--命令查看触发器
DROP TRIGGER trigger_name;--删除
```

###### Oracle

触发器：类似于AOP（面向切面编程）中的拦截器；不能传递参数，输出参数，也不能显示调用，只有满足触发器条件时会由Oracle自动调用。

- **限制**

  1、触发器不接受参数
  2、一个表上最多可有12个触发器，但同一时间、同一事件、同一类型的触发器只能有一个。并各触发器之间不能有矛盾。
  3、一个表上的触发器越多，该表上的DM操作的性能影响就越大
  4、触发器代码的大小不能超过32K。如需要大量的代码创建触发器，则首先创建过程，然后在触发器中使用CALL语句调用过程
  5、触发器代码只能包含SELECT、INSERT、UPDATE和DELETE语句，
  6、不能包含DDL语句(CREATE、ALTER和DROP) 和事务控制语句（COMMIT、ROLLBACK和SAVEPOINT）

- **创建dml触发器**

  **语句触发器**
  1、语句触发器是指当执行DML语句时被隐含执行的触发器
  2、如果在表上针对某种DML操作创建了语句触发器，则当执行DML操作时会自动地执行触发器的相应代码
  3、为了审计DML操作，或者确保DML操作安全执行时，可以使用语句触发器

  触发器用途很多，例如用户清算购物车后将会触发待收货的数据库

```plsql
--创建触发器
create or replace trigger tri_test
before--触发之前
update or delete--更新或删除
on emp
for each row--对行进行操作
  begin
    dbms_output.put_line(:old.sal);--old表示数据库旧值
    insert into demo(id) values (:new.sal);--new新值
  end;

update emp set sal=888 where empno=7788;
commit;
--代码解释：先执行创建触发器代码后，再执行最后的更新语句。当更新恩平、表后将会输出数据库中本来存放的值，并且触发添加语句在demo表中插入一条语句。


--查询
select * from 表 where object_type='TRIGGER';
--删除
drop trigger ...;
drop trigger ...;
```



#### 定时任务：

###### [MySQL](https://blog.csdn.net/chenshun123/article/details/79677193?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.control)

自 MySQL5.1.6起，增加了一个非常有特色的功能–事件调度器(Event Scheduler)，可以用做定时执行某些特定任务，来取代原先只能由操作系统的计划任务来执行的工作。事件调度器有时也可称为临时触发器(temporal triggers)，因为事件调度器是基于特定时间周期触发来执行某些任务，而触发器(Triggers)是基于某个表所产生的事件触发的，区别也就在这里。

```sql
--在使用这个功能之前必须确保 event_scheduler 已开启，可执行 :
mysq> SET GLOBAL event_scheduler = 1;
# 或
mysql> SET GLOBAL event_scheduler = ON;

也可以在配置文件中添加设置 : event_scheduler=1
也可以直接在启动命令加上 : --event_scheduler=1

--查看当前是否已开启事件调度器 :
mysql> SHOW VARIABLES LIKE 'event_scheduler';

# 或
mysql> SELECT @@event_scheduler;

# 或
mysql> SHOW PROCESSLIST;

--创建事件(CREATE EVENT)
CREATE EVENT [IFNOT EXISTS] event_name
  ON SCHEDULE schedule
  [ON COMPLETION [NOT] PRESERVE] --设置这个事件是执行一次还是持久执行，默认为 NOT PRESERVE
  [ENABLE | DISABLE] --可设置该事件创建后状态是否开启或关闭，默认为ENABLE
  [COMMENT 'comment'] --可以给该事件加上注释
  DO sql_statement;

--修改事件(ALTER EVENT)
ALTER EVENT event_name
  [ON SCHEDULE schedule]
  [RENAME TO new_event_name]
  [ON COMPLETION [NOT] PRESERVE]
  [COMMENT 'comment']
  [ENABLE | DISABLE]
  [DO sql_statement]
  
--临时关闭事件
mysql> ALTER EVENT e_test DISABLE;
# 开启事件
mysql> ALTER EVENT e_test ENABLE;

--删除事件(DROP EVENT)
DROP EVENT [IF EXISTS] event_name
```

###### [Oracle](https://blog.csdn.net/qq_40709468/article/details/81876828?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160826139016780277846162%252522%25252C%252522scm%252522%25253A%25252220140713.130102334..%252522%25257D&request_id=160826139016780277846162&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-81876828.nonecase&utm_term=Oracle%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA)

```plsql
--创建job
begin
  sys.dbms_job.submit(job => 1,        --代表的是号码，第几个定时任务
       what => 'sys_mailing_list_job;',  --这个是调用的你想使用的存储过程切记要打；不然会报错
       next_date => to_date('20-08-2018 14:05:00', 'dd-mm-yyyy hh24:mi:ss'),  --这个是下次调用的时间 
       interval => 'trunc(sysdate,''hh'')+(60+5)/(24*60)');
  commit;    --这个是间隔时间 。我这个代表的是每个小时的过5 比如 1:05,2:05,3:05...24小时的
end;

--删除
job: dbms_job.remove(jobno); -- jobno就是你得任务号 

--修改要执行的操作: 
job:dbms_job.what(jobno, what);   --指定任务号以及存储过程

--修改下次执行时间：
dbms_job.next_date(jobno, next_date);  --指定任务号的时间

--修改间隔时间：
dbms_job.interval(jobno, interval);   --指定任务号的间隔时间

--启动job: 
dbms_job.run(jobno);    --指定任务号启动

--停止job: 
dbms.broken(jobno, broken, nextdate); --broken为boolean值 N代表启动，Y代表没启动（STOP）
```



#### [Linux ：](https://blog.csdn.net/weixin_44895651/article/details/105289038?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160825080916780265399902%252522%25252C%252522scm%252522%25253A%25252220140713.130102334..%252522%25257D&request_id=160825080916780265399902&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_yy~default-1-105289038.nonecase&utm_term=linux)

[命令手册](https://blog.csdn.net/p1279030826/article/details/106546972?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160826328016780274045083%252522%25252C%252522scm%252522%25253A%25252220140713.130102334..%252522%25257D&request_id=160826328016780274045083&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-106546972.nonecase&utm_term=linux%E5%91%BD%E4%BB%A4%E6%89%8B%E5%86%8C)

```markdown
运行jar包：java -jar
后台运行jar项目：nohup java -jar babyshark-0.0.1-SNAPSHOT.jar  > log.file  2>&1 &
上面的2 和 1 的意思如下:
0    标准输入（一般是键盘）
1    标准输出（一般是显示屏，是用户终端控制台）
2    标准错误（错误信息输出）
将运行的jar 错误日志信息输出到log.file文件中，然后（>&1）就是继续输出到标准输出(前面加的&，是为了让系统识别是标准输出)，最后一个&,表示在后台运行。
后台进程：netstat -anp
查看日志全文：cat + 日志名称.log

实时查看日志：tail -f +日志名称.log
模糊查询日志：grep -r -200 "查询内容" 日志名称.log

查看进程：
ps命令
-a，查看所有
-u，以用户（user）的格式显示
-x, 显示后台进程运行参数
-ef，以全格式显示进程所有信息，包括父进程Pid，创建人，创建时间，进程号。等等
 ps -l   列出与本次登录有关的进程信息；
 ps -aux   查询内存中进程信息；
 ps -aux | grep ...  查询...进程的详细信息；
 top   查看内存中进程的动态信息；
 kill -9 pid   杀死进程。

1：文件管理
ls命令 – 显示指定工作目录下的内容及属性信息
mkdir命令 – 创建目录
cp命令 – 复制文件或目录
mv命令 – 移动或改名文件
pwd命令 – 显示当前路径
2：文档编辑
cat命令 – 在终端设备上显示文件内容
echo命令 – 输出字符串或提取Shell变量的值
rm命令 – 移除文件或目录
tail命令 – 查看文件尾部内容
rmdir命令 – 删除空目录
3：系统管理
startx命令 – 初始化X-windows
rpm命令 – RPM软件包管理器
vmstat命令 – 显示虚拟内存状态
find命令 – 查找和搜索文件
uname命令 – 显示系统信息
4：磁盘管理
df命令 – 显示磁盘空间使用情况
fdisk命令 – 磁盘分区
hdparm命令 – 显示与设定硬盘参数
lsblk命令 – 查看系统的磁盘
vgextend命令 – 扩展卷组
5：文件传输
tftp命令 – 上传及下载文件
curl命令 – 文件传输工具
fsck命令 – 检查并修复Linux文件系统
ftpwho命令 – 显示ftp会话信息
lprm命令 – 删除打印队列中的打印任务
6：网络通讯
ssh命令 – 安全连接客户端
ping命令 – 测试主机间网络连通性
netstat命令 – 显示网络状态
ifconfig命令 – 显示或设置网络设备
ss命令 – 显示活动套接字信息
7：设备管理
mount命令 – 文件系统挂载
MAKEDEV命令 – 建立设备
setleds命令 – 设定键盘上方三个 LED 的状态
lspci命令 – 显示当前设备所有PCI总线信息
sensors命令 – 检测服务器内部温度及电压
8：备份压缩
zipinfo命令 – 查看压缩文件信息
unarj命令 – 解压.arj文件
gzip命令 – 压缩和解压文件
zip命令 – 压缩文件
unzip命令 – 解压缩zip文件
9：其他命令
hash命令 – 显示与清除命令运行时查询的哈希表
bc命令 – 浮点运算
wait命令 – 等待指令
rmmod命令 – 删除模块
history命令 – 显示与操纵历史命令











```



#### [Docker](https://blog.csdn.net/lqpf199681/article/details/110518692?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160827066716780261944216%252522%25252C%252522scm%252522%25253A%25252220140713.130102334..%252522%25257D&request_id=160827066716780261944216&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-110518692.nonecase&utm_term=Docker)：

##### 一.Docker介绍

> Docker 是一个开源的应用容器引擎，基于 [Go 语言](https://www.runoob.com/go/go-tutorial.html) 并遵从 Apache2.0 协议开源。
>
> Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。
>
> 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。
>
> Docker 从 17.03 版本之后分为 CE（Community Edition: 社区版） 和 EE（Enterprise Edition: 企业版）。

##### 二.Docker的基本操作

##### 2.1基本准备

这篇文章中我的运行环境都是在CentOS7中运行。

配套视频: [2020 Docker最新超详细版教程通俗易懂](https://www.bilibili.com/video/BV1sK4y1s7Cj)

##### 2.2安装Docker

```sh
# 1.下载关于Docker的依赖环境
yum -y install yum-utils device-mapper-persistent-data lvm2
```

------

```sh
# 2.设置下载Docker的镜像源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```

------

```sh
# 3.安装Docker
yum makacache fast
yum -y install docker-ce
```

------

```sh
# 4.启动Docker，并设置为开机自动启动，测试
# 启动Docker服务
systemctl start docker
# 设置开机自动启动
systemctl enable docker
# 测试
docker run hello-world
```

##### 2.3 Docker的中央仓库

> 1. Docker官方的中央仓库: 这个仓库是镜像最全的，但是下载速度较慢。
>    https://hub.docker.com/
> 2. 国内的镜像网站：网易蜂巢、daoCloud。。。
>    https://c.163yun.com/hub#/home
>    https://hub.daocloud.io/ (推荐使用)
> 3. 在公司内部会采用私服的方式拉取镜像。(添加配置)

```json
# 需要在/etc/docker/daemon.json
{
	"registry-mirrors": ["https://registry.docker-cn.com"],
	"insecure-registries": ["ip:port]
}
# ip:port
公司私服的ip和port
# 重启两个服务
systemctl daemon-reload
systemctl restart docker                          
```

##### 2.4 镜像的操作

```sh
# 1. 拉取镜像到本地
docker pull 镜像名称[:tag]
# 举个例子 tomcat
docker pull daocloud.io/library/tomcat:8.5.15-jre8
```

------

```sh
# 2. 查看全部本地的镜像
docker images
```

------

```sh
# 3. 删除本地镜像
docker rmi 镜像的标识
```

------

```sh
# 4. 镜像的导入导出(不规范)
# 将本地的镜像导出
docker save -o 导出的路径 镜像id
# 加载本地的镜像文件
docker load -i 镜像文件
# 修改镜像名称
docker tag 镜像id 新镜像名称:版本
```

##### 2.5 容器的操作

```sh
# 1. 运行容器
# 简单操作
docker run 镜像的标识|镜像名称[tag]
# 常用的参数
docker run -d -p  宿主机端口:容器端口 --name 容器名称 镜像的标识|镜像名称[tag]
# -d: 代表后台运行容器
# -p: 宿主机端口:容器端口: 为了映射当前Linux的端口和容器的端口
# --name 容器名称: 指定容器的名称
```

------

```sh
# 2. 查看正在运行的容器
docker ps [OPTIONS]
# OPTIONS说明:
# -a: 代表查看全部的容器，包括没有运行
# -q: 只查看容器的标识
# -f: 根据条件过滤显示的内容
# --format: 指定返回值的模板文件
# -l: 显示最近创建的容器
# -n: 列出最近创建的n个容器
# --no-trunc: 不截断输出
# -s: 显示总的文件大小
```

------

```sh
# 3. 查看容器的日志
docker logs -f 容器id
# -f: 可以滚动查看日志的最后几行
```

------

```sh
# 4. 进入到容器内部
docker exec -it 容器id bash
```

------

```sh
# 5. 删除容器(删除容器前，需要先停止容器)
docker stop 容器id
# 停止指定的容器
docker stop $(docker ps -qa)
# 停止全部容器
docker rm 镜像id
# 删除指定容器
docker rm $(docker ps -qa)
# 删除全部容器
```

------

```sh
#6. 启动容器
docker start 容器id
```

##### 三.Docker应用

##### 3.1 准备SSM工程

```sh
# MySQL数据库的连接用户名和密码改变了，修改db.propreties
# 项目重新打包
mvn clean package -DskipTests
# 项目地址
链接: https://pan.baidu.com/s/1F4xTLoOFCMb7rl1VUrBASA  密码: bgjw
```

##### 3.2 准备MySQL容器

```sh
# 运行MySQL容器
docker run -d -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=root daocloud.io/library/mysql:5.7.4
```

##### 3.3 准备Tomcat容器

```sh
# 运行Tomcat容器，在上面容器操作中已经搞定，只需要将SSM项目的war包部署到Tomcat容器内部
# 可以通过命令将宿主机的内容复制到容器内部
docker cp 文件名称 容器id:容器内部路径
# 举个例子
docker cp ssm.war fe:/usr/local/tomcat/webapps/
```

##### 3.4数据卷

> 为了部署SSM的工程，需要使用到cp的命令将宿主机内的ssm.war文件复制到容器内部。
>
> 数据卷：将宿主机的一个目录映射到容器的一个目录中。
>
> 可以在宿主机中操作目录中的内容，那么容器内部映射的文件，也会跟着一起改变。

```sh
# 1. 创建数据卷
docker volume create 数据卷名称
# 创建数据卷之后默认会存放在一个目录下 /var/lib/docker/volumes/数据卷名称/_data
```

------

```sh
# 2. 查看数据卷的详细信息
docker volume inspect 数据卷名称
```

------

```sh
# 3. 查看全部数据卷
docker volume ls
```

------

```sh
# 4. 删除数据卷
docker volume rm 数据卷名称
```

------

```sh
# 5. 应用数据卷
# 当你映射数据卷时，如果数据卷不存在。Docker会帮你自动创建
docker run -v 数据卷名称:容器内部路径 镜像id
# 直接指定一个路径作为数据卷的存放位置。这个路径下是空的。
docker run -v 路径:容器内部的路径 镜像id
```

##### 四.Docker自定义镜像

> 中央仓库上的镜像，也是Docker的用户自己上传过去的。

```sh
# 1. 创建一个Dockerfile文件，并且指定自定义镜像信息。
# Dockerfile文件中常用的内容
from: 指定当前自定义镜像依赖的环境
copy: 将相对路径下的内容复制到自定义镜像中
workdir: 声明镜像的默认工作目录
cmd: 需要执行的命令(在workdir下执行的，cmd可以写多个，只以最后一个为准)
# 举个例子，自定义一个tomcat镜像，并且将ssm.war部署到tomcat中
from daocloud.io/library/tomcat:8.5.15-jre8
copy ssm.war /usr/local/tomcat/webapps
```

------

```sh
# 2. 将准备好的Dockerfile和相应的文件拖拽到Linux操作系统中，通过Docker的命令制作镜像
docker build -t 镜像名称:[tag] .
```

##### 五.Docker-Compose

> 之前运行一个镜像，需要添加大量的参数。
>
> 可以通过Docker-Compose编写这些参数。
>
> Docker-Compose可以帮助我们批量的管理容器。
>
> 只需要通过一个docker-compose.yml文件去维护即可。

##### 5.1 下载Docker-Compose

```sh
# 1. 去Github官网搜索docker-compose，下载1.24.1版本的Docker-Compose
https://github.com/docker/compose/releases/download/1.24.1/docker-compose-Linux-x86_64

# 2. 将下载好的文件拖拽到Linux系统中
将文件上传到你所使用的服务器或者虚拟机，然后将文件移动到/usr/local

# 3. 需要将Docker-Compose文件的名称修改一下，基于Docker-Compose文件一个可执行的权限
mv docker-compose-Linux-x86_64 docker-compose
chmod 777 docker-compose

# 4. 方便后期操作，配置一个环境变量
# 将docker-compose文件移动到了/usr/local/bin，修改了/etc/profile文件，给/usr/local/bin配置到了PATH中
mv docker-compose /usr/local/bin
vi /etc/profile
	export PATH=/usr/local/bin:$PATH
source /etc/profile

# 5. 测试一下
# 在任意目录下输入docker-compose
```

![测试结果](https://img-blog.csdnimg.cn/img_convert/b6e4b1a31be51f5269ddda3ac195dbe8.png)

##### 5.2 Docker-Compose管理MySQL和Tomcat容器

> yml文件以key:value方式来指定配置信息
>
> 多个配置信息以换行+缩进的方式来区分
>
> 在docker-compose.yml文件中，不要使用制表符

```yml
version: '3.1'
services:
  mysql:                     # 服务的名称
    restart: always          # 代表只要Docker启动，那么这个容器就跟着一起启动
    image: daocloud.io/library/mysql:5.7.4     # 指定镜像路径
    container_name: mysql    # 指定容器名称
    ports:
      - 3306:3306        # 指定端口号的映射
    environment:
      MYSQL_ROOT_PASSWORD: root         # 指定MySQL的ROOT用户登录密码
      TZ: Asia/Shanghai                 # 指定时区
    volumes:
      - /opt/docker_mysql_tomcat/mysql_data:/var/lib/mysql        # 映射数据卷
  tomcat:
    restart: always          # 代表只要Docker启动，那么这个容器就跟着一起启动
    image: daocloud.io/library/tomcat:8.5.15-jre8     # 指定镜像路径
    container_name: tomcat    # 指定容器名称
    ports:
      - 8080:8080        # 指定端口号的映射
    environment:
      MYSQL_ROOT_PASSWORD: root         # 指定MySQL的ROOT用户登录密码
      TZ: Asia/Shanghai                 # 指定时区
    volumes:
      - /opt/docker_mysql_tomcat/tomcat_webapps:/usr/local/tomcat/webapps        # 映射数据卷
      - /opt/docker_mysql_tomcat/tomcat_logs:/usr/local/tomcat/logs        # 映射数据卷
```

##### 5.3 使用docker-compose命令管理容器

> 在使用docker-compose的命令时，默认会在当前目录下找docker-compose.yml

```sh
# 1. 基于docker-compose.yml启动管理的容器
docker-compose up -d
```

------

```sh
# 2. 关闭并删除容器
docker-compose down
```

------

```sh
# 3. 开启 | 关闭 | 重启已经存在的由docker-compose维护的容器
docker-compose start | stop | restart
```

------

```sh
# 4. 查看由docker-compose管理的容器
docker-compose p
```

------

```sh
# 5. 查看日志
docker-compose logs -f
```

##### 5.4 docker-compose配置Dockerfile使用

> 使用docker-compose.yml文件以及Dockerfile文件在生产自定义镜像的同时启动当前镜像，并且由docker-compose去管理容器

[docker-compose.yml](https://blog.csdn.net/lqpf199681/article/details/110518692?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160827066716780261944216%252522%25252C%252522scm%252522%25253A%25252220140713.130102334..%252522%25257D&request_id=160827066716780261944216&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-110518692.nonecase&utm_term=Docker)

```yml
# yml文件
version: '3.1'
services:
  ssm:
    restart: always
    build:          # 构建自定义镜像
      context: ../. # 指定dockerfile文件的所在路径
      dockerfile: Dockerfile  # 指定Dockerfile文件名称
    image: ssm:1.0.1
    container_name: ssm
    ports:
      - 8081:8080
    environment:
      TZ: Asia/Shanghai
```

------

[Dockerfile文件](https://blog.csdn.net/lqpf199681/article/details/110518692?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160827066716780261944216%252522%25252C%252522scm%252522%25253A%25252220140713.130102334..%252522%25257D&request_id=160827066716780261944216&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-110518692.nonecase&utm_term=Docker)

```sh
from daocloud.io/library/tomcat:8.5.15-jre8
copy ssm.war /usr/local/tomcat/webapps
```

------

```sh
# 可以直接启动基于docker-compose.yml以及Dockerfile文件构建的自定义镜像
dockr-compose up -d
# 如果自定义镜像不存在，会帮助我们构建出自定义镜像，如果自定义镜像已经存在，会直接运行这个自定义镜像
# 重新构建的话
# 重新构建自定义镜像
docker-compose build
# 运行前，重新构建
docker-compose up -d --build
```

##### 六.Docker DI、CD

##### 6.1 引言

> 项目部署
>
> 1. 将项目通过maven进行编译打包
> 2. 将文件上传到指定的服务器中
> 3. 将war包放到tomcat的目录中
> 4. 通过Dockerfile将Tomcat和war包转成一个镜像，由DockerCompose去运行容器
>
> 项目更新了
>
>  将上述流程再次的从头到尾的执行一次

##### 6.2 CI介绍

> CI(continuous intergration)持续集成
>
> 持续集成：编写代码时，完成了一个功能后，立即提交代码到Git仓库中，将项目重新的构建并进行测试。
>
> - 快递发现错误。
> - 防止代码偏离主分支。

##### 6.3 实现持续集成

##### 6.3.1 搭建Gitlab服务器

> 1、创建一个全新的虚拟机，并且至少指定4G的运行内存

> 2、安装docker以及docker-compose

> 3、将ssh的默认22端口，修改为60022端口

```sh
vi /etc/ssh/sshd_config
Port 22 -> 60022
systemctl restart sshd
```

> 4、docker-compose.yml文件去安装Gitlab(下载和运行的时间比较长)

```yml
version: '3.1'
services:
  gitlab:
    image: 'twang2218/gitlab-ce-zh:11.1.4'
    container_name: 'gitlab'
    restart: always
    privileged: true
    hostname: 'gitlab'
    environment:
      TZ: 'Asia/Shanghai'
      GITLAB_OMNIBUS_CONFIG: |
        external_url 'http://192.168.199.110'
        gitlab_rails['time_zone'] = 'Asia/Shanghai'
        gitlab_rails['smtp_enable'] = true
        gitlab_rails['gitlab_shell_ssh_port'] = 22
    ports:
      - '80:80'
      - '443:443'
      - '22:22'
    volumes:
      - /opt/docker_gitlab/config:/etc/gitlab
      - /opt/docker_gitlab/data:/var/opt/gitlab
      - /opt/docker_gitlab/logs:/var/log/gitlab
```

##### 6.3.2 搭建Gitlab-Runner

> 查看资料中的g i t la b-runner文件即可安装

------

##### 6.3.3 整合项目入门测试

> 1、创建一个maven工程，添加web.xml文件，编写html页面

> 2、编写gitlab-ci.yml文件

```
stages:
  - test

test:
  stage: test
  script:
    - echo first test ci # 输入的命令
```

> 3、将maven工程推送到gitlab中

> 4、可以在gitlab中查看到gitlab-ci.yml编写的内容

```
打开gitlab控制台-左侧CI/CD-流水线-已通过
```

![查看](https://img-blog.csdnimg.cn/img_convert/e7328d8de89f3084209ef9bbef77ac5a.png)

------

##### 6.3.4编写.gitlab-ci.yml文件

> 1、编写.gitlab-ci.yml测试命令使用

```yml
stages:
  - test

test:
  stage: test
  script:
    - echo first test ci
    - /usr/local/maven/apache-maven-3.6.3/bin/mvn package
```

> 2、编写关于dockerfile以及dock er-compose.yml文件的具体内容

```sh
# 1. Dockerfile
FROM daocloud.io/library/tomcat:8.5.15-jre8
COPY testci.war /usr/local/tomcat/webapps
```

------

```sh
# 2. docker-compose.yml
version: '3.1'
services:
  testci:
    build: docker
    restart: always
    container_name: testci
    ports:
    - 8080:8080
```

------

```sh
# 3. ci.yml
stages:
  - test

test:
  stage: test
  script:
    - echo first test ci
    - /usr/local/maven/apache-maven-3.6.3/bin/mvn package
    - cp target/testci-1.0-SNAPSHOT.war docker/testci.war
    - docker-compose down
    - docker-compose up -d --build
    - docker rmi $(docker images -qf dangling=true)
```

> 3、测试

![测试图](https://img-blog.csdnimg.cn/img_convert/1f970fad891bbf11c0675b5bde29a1f8.png)

##### 6.4 CD介绍

> CD（持续交付，持续部署）

##### 6.5 实现持续交付持续部署

##### 6.5.1 安装Jenkins

> 官网：https://www.jenkins.io/

```yml
version: '3.1'
services:
  jeckins:
    image: jenkins/jenkins
    restart: always
    container_name: jenkins
    ports:
      - 8888:8080
      - 50000:50000
    volumes:
      - ./data:/var/jenkins_home
```

> 第一次运行时，会因为data目录没有权限，导致启动失败

```
chmod 777 data
```

> 访问http://192.168.199.109:8888

```
访问速度奇慢无比。。。。
```

> 输入密码

![输入密码](https://img-blog.csdnimg.cn/img_convert/50d151bb40d187bfca7361df00caefd6.png)

> 手动指定插件安装：指定下面两个插件即可
>
> Publish ssh
>
> git param…

这里安装过程忘了截图了。。。因为服务器安装的太快，没反应过来

> 安装成功后，需要指定用户名和密码

![jenkins控制台](https://img-blog.csdnimg.cn/img_convert/0569a30976767303c909bdcf8d4c05fe.png)

------

##### 6.5.2 配置目标服务器以及Gitlab免密码登录

> Gitlab -> Jenkins -> 目标服务器

> 1、Jenkins去连接目标服务器

> 左侧的系统设置

![image-20201202103359125](https://img-blog.csdnimg.cn/img_convert/832c09e7741c628004a42a00856b2c9b.png)

> 选中系统设置

![image-20201202135154813](https://img-blog.csdnimg.cn/img_convert/57e17ad2fbf2a0c2a69adfe8f016ac96.png)

> 搜索Publish over SSH

![image-20201202135435348](https://img-blog.csdnimg.cn/img_convert/6c1860c62976a40963ff40a0532087ee.png)

> 点击新增

![image-20201202150053909](https://img-blog.csdnimg.cn/img_convert/a3ea72cb8050d2158513534bd9d0ff9c.png)

------

##### 6.5.3 配置Gitlab免密码登录

> 1、登录Jenkins容器内部

```
docker exec -it jenkins bash
```

> 2、输入生成SSH密钥命令

```
ssh-keygen -t rsa -C "邮箱"
```

> 3、将密钥复制到Gitlab的SSH中

![image-20201202151117361](https://img-blog.csdnimg.cn/img_convert/4544b08b84c5a09b560549e11485adbd.png)

------

##### 6.5.4 配置JDK和Maven

> 1、复制本地的jdk和maven的压缩包到data目录下

> 2、手动解压

![image-20201202152610797](https://img-blog.csdnimg.cn/img_convert/bc619ae29583dbba18e173717f1f081f.png)

> 3、在监控界面中配置JDK和Maven

![image-20201202152453378](https://img-blog.csdnimg.cn/img_convert/e941a6829ca5a008f79a0396906bbbda.png)

------

##### 6.5.5 手动拉取gitlab项目

> 使用SSH无密码连接时，第一次连接需要手动确定

![image-20201202153041926](https://img-blog.csdnimg.cn/img_convert/f85405393acc6f89f84dd98b86af3f8c.png)

------

##### 6.5.6 创建maven任务

> 1、创建maven工程，推送到gitlab

> 2、jenkins的监控页面中创建maven任务

![image-20201202161727559](https://img-blog.csdnimg.cn/img_convert/a7e2c48e542e6ebdb46d092617ac1ab2.png)

![image-20201202161756699](https://img-blog.csdnimg.cn/img_convert/eac1133874e9ee8bd7d591188d9b20d7.png)

> 3、执行maven任务

![image-20201202162242988](https://img-blog.csdnimg.cn/img_convert/9afd33fdcbc2b80d862fa7e03883d2aa.png)

![image-20201202162104630](https://img-blog.csdnimg.cn/img_convert/f7d470cdc6aee5fb8867f3cf31e611d0.png)

> 4、最终效果

![image-20201202165449820](https://img-blog.csdnimg.cn/img_convert/deaef3b93b9ae40c23e374b43fc27c51.png)

##### 6.6 实现持续交付持续部署

> 1、安装Git Parameter的插件，Persistent Parameter的插件（版本）

![image-20201202165935326](https://img-blog.csdnimg.cn/img_convert/22deb5f69bac7ab0a8d23bbd8759b957.png)

> 2、重新制定构建项目的方式

[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-JmlV9ILd-1606958453454)(/Users/liufei/Library/Application Support/typora-user-images/image-20201202224449925.png)]

------

![image-20201203000655114](https://img-blog.csdnimg.cn/img_convert/dde373a08b9a86fd5ed11c7395f1c8dd.png)

> 3、构建项目成功后，需要将内容发布到目标服务器

![image-20201203000749347](https://img-blog.csdnimg.cn/img_convert/1e7cbbb702eb1fe31602d59eec0482a2.png)

> 4、修改程序代码，（提交到GitLab仓库中）

```
FROM daocloud.io/library/tomcat:8.5.15-jre8
COPY testcd-1.0-SNAPSHOT.war /usr/local/tomcat/webapps
```

------

```
version: '3.1'
services:
  testcd:
    build: docker
    restart: always
    container_name: testcd
    ports:
      - 8081:8080
```

> 5、测试
>
> 1. 给当前代码添加一个标签
> 2. 到Jenkins中查看

![image-20201203001600023](https://img-blog.csdnimg.cn/img_convert/8d58e2769405c6960726610769681299.png)

> 1. 点击上图的开始构建(查看日志)
> 2. 去指定的目标服务器中访问具体服务

#### 多线程：

##### 1. 并行和并发有什么区别？

1. 并行（Parallel）：指两个或者多个事件在同一时刻发生，即同时做不同事的能力。例如垃圾回收时，多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。
2. 并发（Concurrent）：指两个或多个事件在同一时间间隔内发生，即交替做不同事的能力，多线程是并发的一种形式。例如垃圾回收时，用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。

##### 2. 线程和进程的基本概念、线程的基本状态以及状态之间的关系？

1. 一个线程是进程的一个顺序执行流程。一个进程中的全部线程共享同一个堆空间。线程本身有一个供程序执行时的栈，一个进程中可以包含多个线程。
2. 线程的**基本状态**：新建、就绪、运行状态、阻塞状态、死亡状态
3. 新建状态：利用NEW运算创建了线程对象，此时线程状态为新建状态，调用了新建状态线程的start()方法，将线程提交给操作系统，准备执行，线程将进入到就绪状态。
4. 就绪状态：由操作系统调度的一个线程，没有被系统分配到处理器上执行，一旦处理器有空闲，操作系统会将它放入处理器中执行，此时线程从就绪状态切换到运行时状态。
5. 运行状态：线程正在运行的过程中，碰到调用Sleep()方法，或者等待IO完成，或等待其他同步方法完成时，线程将会从运行状态，进入到阻塞状态。
6. 死亡状态：线程一旦脱离阻塞状态时，将重新回到就绪状态，重新向下执行，最终进入到死亡状态。一旦线程对象是死亡状态，就只能被GC回收，不能再被调用。

##### 3. 守护线程是什么？

1. 守护线程又称为后台线程，它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件
2. 正常创建的线程都是普通线程，或称为前台线程，守护线程与普通线程在使用上没有什么区别，但是他们有一个最主要的区别是在于进程的结束中。当一个进程中所有普通线程都结束时，那么进程就会结束。如果进程结束时还有守护线程在运行，那么这些守护线程就会被强制结束
3. 在 Java 中垃圾回收线程就是特殊的守护线程

##### 4. 创建线程有哪几种方式？

1. 继承Thread类（真正意义上的线程类），是Runnable接口的实现。
2. 实现Runnable接口，并重写里面的run方法。
3. 使用Executor框架创建线程池。Executor框架是juc里提供的线程池的实现。

##### 5. sleep() 和 wait() 有什么区别？

1. 类的不同：sleep() 来自 Thread，wait() 来自 Object。
2. 释放锁：sleep() 不释放锁；wait() 释放锁。
3. 用法不同：sleep() 时间到会自动恢复；wait() 可以使用 notify()/notifyAll()直接唤醒。

##### 6. 线程的 run() 和 start() 有什么区别？

1. start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。
2. run() 可以重复调用，而 start() 只能调用一次。
3. 第二次调用start() 必然会抛出运行时异常

##### 7. 创建线程池有哪几种方式？

1. newSingleThreadExecutor()：它的特点在于工作线程数目被限制为 1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目；
2. newCachedThreadPool()：它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过 60 秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用 SynchronousQueue 作为工作队列；
3. newFixedThreadPool(int nThreads)：重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有 nThreads 个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目 nThreads；
4. newSingleThreadScheduledExecutor()：创建单线程池，返回 ScheduledExecutorService，可以进行定时或周期性的工作调度；
5. newScheduledThreadPool(int corePoolSize)：和newSingleThreadScheduledExecutor()类似，创建的是个 ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程；
6. newWorkStealingPool(int parallelism)：这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序；
7. ThreadPoolExecutor()：是最原始的线程池创建，上面1-3创建方式都是对ThreadPoolExecutor的封装。

##### 8. 在 Java 程序中怎么保证多线程的运行安全？

1. 使用安全类，比如 Java. util. concurrent 下的类。
2. 使用自动锁 synchronized。
3. 使用手动锁 Lock。

1.Vector

Vector和ArrayList类似，是长度可变的数组，与ArrayList不同的是，Vector是线程安全的，它给几乎所有的public方法都加上了synchronized关键字。由于加锁导致性能降低，在不需要并发访问同一对象时，这种强制性的同步机制就显得多余，所以现在Vector已被弃用

2.HashTable

HashTable和HashMap类似，不同点是HashTable是线程安全的，它给几乎所有public方法都加上了synchronized关键字，还有一个不同点是HashTable的K，V都不能是null，但HashMap可以，它现在也因为性能原因被弃用了

二、Collections包装方法

Vector和HashTable被弃用后，它们被ArrayList和HashMap代替，但它们不是线程安全的，所以Collections工具类中提供了相应的包装方法把它们包装成线程安全的集合

```
List<E> synArrayList = Collections.synchronizedList(new ArrayList<E>());

Set<E> synHashSet = Collections.synchronizedSet(new HashSet<E>());

Map<K,V> synHashMap = Collections.synchronizedMap(new HashMap<K,V>());

...1234567
```

Collections针对每种集合都声明了一个线程安全的包装类，在原集合的基础上添加了锁对象，集合中的每个方法都通过这个锁对象实现同步

三、java.util.concurrent包中的集合

1.ConcurrentHashMap

ConcurrentHashMap和HashTable都是线程安全的集合，它们的不同主要是加锁粒度上的不同。HashTable的加锁方法是给每个方法加上synchronized关键字，这样锁住的是整个Table对象。而ConcurrentHashMap是更细粒度的加锁 
在JDK1.8之前，ConcurrentHashMap加的是分段锁，也就是Segment锁，每个Segment含有整个table的一部分，这样不同分段之间的并发操作就互不影响 
JDK1.8对此做了进一步的改进，它取消了Segment字段，直接在table元素上加锁(乐观锁CAS算法)，实现对每一行进行加锁，进一步减小了并发冲突的概率

2.CopyOnWriteArrayList和CopyOnWriteArraySet

它们是加了写锁的ArrayList和ArraySet，锁住的是整个对象，但读操作可以并发执行

除此之外还有ConcurrentSkipListMap、ConcurrentSkipListSet、ConcurrentLinkedQueue、ConcurrentLinkedDeque等，至于为什么没有ConcurrentArrayList，原因是无法设计一个通用的而且可以规避ArrayList的并发瓶颈的线程安全的集合类，只能锁住整个list，这用Collections里的包装类就能办到

##### 9. 什么是死锁？怎么防止死锁？

1. 当线程 A 持有独占锁a，并尝试去获取独占锁 b 的同时，线程 B 持有独占锁 b，并尝试获取独占锁 a 的情况下，就会发生 AB 两个线程由于互相持有对方需要的锁，而发生的阻塞现象，我们称为死锁。
2. 防止死锁方法：
   1. 尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。
   2. 尽量使用 Java. util. concurrent 并发类代替自己手写锁。
   3. 尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。
   4. 尽量减少同步的代码块。

##### 10. synchronized 和 volatile 的区别是什么？

1. volatile 是变量修饰符；synchronized 是修饰类、方法、代码段。
2. volatile 仅能实现变量的修改可见性，不能保证原子性；而 synchronized 则可以保证变量的修改可见性和原子性。
3. volatile 不会造成线程的阻塞；synchronized 可能会造成线程的阻塞。

##### 11. synchronized 和 Lock 有什么区别？

1. synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。
2. synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。
3. 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。

##### 12. synchronized 和 ReentrantLock 区别是什么？

1. ReentrantLock 使用起来比较灵活，但是必须有释放锁的配合动作；
2. ReentrantLock 必须手动获取与释放锁，而 synchronized 不需要手动释放和开启锁；
3. ReentrantLock 只适用于代码块锁，而 synchronized 可用于修饰方法、代码块等。

##### 13. 为什么使用线程池？

由于创建和销毁线程都需要很大的开销，运用线程池就可以大大的缓解这些内存开销很大的问题；可以根据系统的承受能力，调整线程池中工作线线程的数目，防止因为消耗过多的内存。

#### [JVM：](https://blog.csdn.net/xiaofeng10330111/article/details/105360974?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522160825063116780265356822%252522%25252C%252522scm%252522%25253A%25252220140713.130102334..%252522%25257D&request_id=160825063116780265356822&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-105360974.nonecase&utm_term=jvm)

JDK8 之前的内存区域图如下:

![img](https://img-blog.csdnimg.cn/2020041119404981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9mZW5nMTAzMzAxMTE=,size_16,color_FFFFFF,t_70)

JDK8 之后的内存区域图如下:

![img](https://img-blog.csdnimg.cn/20200411194152415.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9mZW5nMTAzMzAxMTE=,size_16,color_FFFFFF,t_70)

Java 8 中 PermGen 为什么被移出 HotSpot JVM 了？两个主要原因:

1. 由于 PermGen 内存经常会溢出，引发恼人的 *java.lang.OutOfMemoryError: PermGen*，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM
2. 移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。
   根据上面的各种原因，PermGen 最终被移除，**方法区移至 Metaspace，字符串常量移至 Java Heap**。

##### 运行时数据区：

| 序号 | 区域名称     | 共享     | 作用                                                         | 异常                                                         | 备注                                                         |
| ---- | ------------ | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | 程序计数器   | 线程私有 | 记录当前线程所执行的字节码行号指示器。                       | Java虚拟机规范中唯一一个没有规定OutOfMemoryError(内存不足错误)的区域。 | --                                                           |
| 2    | Java虚拟机栈 | 线程私有 | 存放局部变量表、操作数据栈、动态链接、方法出口等信息。       | 栈深大于允许的最大深度，抛出StackOverflowError(栈溢出错误)。 内存不足时，抛出OutOfMemoryError(内存不足错误)。 | 常说的“栈”说的就是Java虚拟机栈，或者是Java虚拟机栈中的局部变量表。 |
| 3    | 本地方法栈   | 线程私有 | 和Java虚拟机栈类似，不过是为JVM用到的Native方法服务。        | 同上                                                         | --                                                           |
| 4    | Java堆       | 线程共享 | 存放实例化数据。                                             | 内存不足时，抛出OutOfMemoryError(内存不足错误)。             | 通过-Xmx和-Xms控制大小。 GC的主要管理对象。                  |
| 5    | 方法区       | 线程共享 | 存放类信息（版本、字段、方法、接口等）、常量、静态变量、即时编译后的代码等数据。 | 内存不足时，抛出OutOfMemoryError(内存不足错误)。             | --                                                           |
| 6    | 运行时常量池 | 线程共享 | 存放编译期生成的各种字面量和符号引用。                       | 内存不足时，抛出OutOfMemoryError(内存不足错误)。             | 属于“方法区”的一部分。                                       |
| 7    | 直接内存     | --       | 如NIO可以使用Native函数库直接分配堆外内存，该内存受计算机内存限制。 | 内存不足时，抛出OutOfMemoryError(内存不足错误)。             | 不是JVM运行时数据区的一部分，也不是JVM虚拟机规范中定义的内存区域。但这部分内存也被频繁的使用。所以放到一起。 |

JVM运行时会分配好方法区和堆，而JVM每遇到一个线程，就为其分配一个程序计数器、Java栈、本地方法栈，当线程终止时，三者（程序计数器、Java栈、本地方法栈）所占用的内存空间也会释放掉。

程序计数器、Java栈、本地方法栈的生命周期与所属线程相同，而方法区和堆的生命周期与JAVA程序运行生命周期相同，所以gc只发生在线程共享的区域（大部分发生在Heap上）。

1、方法区：

有时候也称为永久代（Permanent Generation），在方法区中，存储了每个类的信息（包括类的名称、修饰符、方法信息、字段信息）、类中静态变量、类中定义为final类型的常量、类中的Field信息、类中的方法信息以及编译器编译后的代码等。当开发人员在程序中通过Class对象中的getName、isInterface等方法来获取信息时，这些数据都来源于方法区域，同时方法区域也是全局共享的，在一定的条件下它也会被GC，在这里进行的GC主要是方法区里的常量池和类型的卸载。当方法区域需要使用的内存超过其允许的大小时，会抛出OutOfMemory的错误信息。

在方法区中有一个非常重要的部分就是运行时常量池，用于存放静态编译产生的字面量和符号引用。运行时生成的常量也会存在这个常量池中，比如String的intern方法。它是每一个类或接口的常量池的运行时表示形式，在类和接口被加载到JVM后，对应的运行时常量池就被创建出来。

2、堆：

Java中的堆是用来存储对象实例以及数组（当然，数组引用是存放在Java栈中的）。堆是被所有线程共享的，因此在其上进行对象内存的分配均需要进行加锁，这也导致了new对象的开销是比较大的。在JVM中只有一个堆。堆是Java垃圾收集器管理的主要区域，Java的垃圾回收机制会自动进行处理。

Sun Hotspot JVM为了提升对象内存分配的效率，对于所创建的线程都会分配一块独立的空间TLAB（Thread Local Allocation Buffer），其大小由JVM根据运行的情况计算而得，在TLAB上分配对象时不需要加锁，因此JVM在给线程的对象分配内存时会尽量的在TLAB上分配，在这种情况下JVM中分配对象内存的性能和C基本是一样高效的，但如果对象过大的话则仍然是直接使用堆空间分配。

堆空间分为老年代和年轻代。刚创建的对象存放在年轻代，而老年代中存放生命周期长久的实例对象。年轻代中又被分为Eden区和两个Survivor区(From Space和To Space)。新的对象分配是首先放在Eden区，Survivor区作为Eden区和Old区的缓冲，在Survivor区的对象经历若干次GC仍然存活的，就会被转移到老年代。 当一个对象大于eden区而小于old区（老年代）的时候会直接扔到old区。 而当对象大于old区时，会直接抛出OutOfMemoryError（OOM）。

3、Java栈：

Java栈也称作虚拟机栈（Java Vitual Machine Stack），也就是我们常常所说的栈。JVM栈是线程私有的，每个线程创建的同时都会创建自己的JVM栈，互不干扰。

![img](https://img-blog.csdnimg.cn/20200411194730271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW9mZW5nMTAzMzAxMTE=,size_16,color_FFFFFF,t_70)

Java栈是Java方法执行的内存模型。Java栈中存放的是一个个的栈帧，每个栈帧对应一个被调用的方法，在栈帧中包括局部变量表(Local Variables)、操作数栈(Operand Stack)、指向当前方法所属的类的运行时常量池的引用(Reference to runtime constant pool)、方法返回地址(Return Address)和一些额外的附加信息。当线程执行一个方法时，就会随之创建一个对应的栈帧，并将建立的栈帧压栈。当方法执行完毕之后，便会将栈帧出栈。因此可知，线程当前执行的方法所对应的栈帧必定位于Java栈的顶部。

局部变量表：用来存储方法中的局部变量（包括在方法中声明的非静态变量以及函数形参）。对于基本数据类型的变量，则直接存储它的值，对于引用类型的变量，则存的是指向对象的引用。局部变量表的大小在编译期就可以确定其大小了，因此在程序执行期间局部变量表的大小是不会改变的。

i++ 和 ++i 的区别：

1. i++：从局部变量表取出 i 并压入操作栈，然后对局部变量表中的 i 自增 1，将操作栈栈顶值取出使用，最后，使用栈顶值更新局部变量表，如此线程从操作栈读到的是自增之前的值。
2. ++i：先对局部变量表的 i 自增 1，然后取出并压入操作栈，再将操作栈栈顶值取出使用，最后，使用栈顶值更新局部变量表，线程从操作栈读到的是自增之后的值。

之前之所以说 i++ 不是原子操作，即使使用 volatile 修饰也不是线程安全，就是因为，可能 i 被从局部变量表（内存）取出，压入操作栈（寄存器），操作栈中自增，使用栈顶值更新局部变量表（寄存器更新写入内存），其中分为 3 步，volatile 保证可见性，保证每次从局部变量表读取的都是最新的值，但可能这 3 步可能被另一个线程的 3 步打断，产生数据互相覆盖问题，从而导致 i 的值比预期的小。

操作数栈：栈最典型的一个应用就是用来对表达式求值。在一个线程执行方法的过程中，实际上就是不断执行语句的过程，而归根到底就是进行计算的过程。因此可以这么说，程序中的所有计算过程都是在借助于操作数栈来完成的。

指向运行时常量池的引用：因为在方法执行的过程中有可能需要用到类中的常量，所以必须要有一个引用指向运行时常量。

方法返回地址：当一个方法执行完毕之后，要返回之前调用它的地方，因此在栈帧中必须保存一个方法返回地址。

4、程序计数器：

程序计数器（Program Counter Register），也有称作为PC寄存器。

由于在JVM中，多线程是通过线程轮流切换来获得CPU执行时间的，因此，在任一具体时刻，一个CPU的内核只会执行一条线程中的指令，因此，为了能够使得每个线程都在线程切换后能够恢复在切换之前的程序执行位置，每个线程都需要有自己独立的程序计数器，并且不能互相被干扰，否则就会影响到程序的正常执行次序。因此，可以这么说，程序计数器是每个线程所私有的。

在JVM规范中规定，如果线程执行的是非native（本地）方法，则程序计数器中保存的是当前需要执行的指令的地址；如果线程执行的是native方法，则程序计数器中的值是undefined。

由于程序计数器中存储的数据所占空间的大小不会随程序的执行而发生改变，因此，对于程序计数器是不会发生内存溢出现象(OutOfMemory)的。

5、本地方法栈：

JVM采用本地方法栈来支持native方法的执行，此区域用于存储每个native方法调用的状态。本地方法栈与Java栈的作用和原理非常相似。区别只不过是Java栈是为执行Java方法服务的，而本地方法栈则是为执行本地方法（Native Method）服务的。在JVM规范中，并没有对本地方法栈的具体实现方法以及数据结构作强制规定，虚拟机可以自由实现它。在HotSopt虚拟机中直接就把本地方法栈和Java栈合二为一。

**JDK8 之前，Hotspot 中方法区的实现是永久代（Perm），JDK8 开始使用元空间（Metaspace），以前永久代所有内容的字符串常量移至堆内存，其他内容移至元空间，元空间直接在本地内存分配。**

为什么要使用元空间取代永久代的实现？

- 字符串存在永久代中，容易出现性能问题和内存溢出。
- 类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。
- 永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。
- 将 HotSpot 与 JRockit 合二为一。

补充内容：

- 运行时常量池

运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。

一般来说，除了保存 Class 文件中描述的符号引用外，还会把翻译出来的直接引用也存储在运行时常量池中。

运行时常量池相对于 Class 文件常量池的另外一个重要特征是具备动态性，Java 语言并不要求常量一定只有编译期才能产生，也就是并非预置入 Class 文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用得比较多的便是 String 类的 intern() 方法。

既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。

- 直接内存

直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是 Java 虚拟机规范中定义的内存区域。

在 JDK 1.4 中新加入了 NIO，引入了一种基于通道（Channel）与缓冲区（Buffer）的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据。

显然，本机直接内存的分配不会受到 Java 堆大小的限制，但是，既然是内存，肯定还是会受到本机总内存（包括 RAM 以及 SWAP 区或者分页文件）大小以及处理器寻址空间的限制。服务器管理员在配置虚拟机参数时，会根据实际内存设置 -Xmx 等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统级的限制），从而导致动态扩展时出现 OutOfMemoryError 异常。

##### 类加载机制

Class文件由类装载器装载后，在JVM中将形成一份描述Class结构的元信息对象，通过该元信息对象可以获知Class的结构信息：如构造函数，属性和方法等，Java允许用户借由这个Class相关的元信息对象间接调用Class对象的功能。

虚拟机把描述类的数据从class文件加载到内存，并对数据进行校验，转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。

![1608273106769](D:\Desktop\Markdown\拓展\拓展.assets\1608273106769.png)

##### 垃圾回收机制



##### 内存溢出

内存泄漏memory leak :是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄漏似乎不会有大的影响，但内存泄漏堆积后的后果就是内存溢出。 

内存溢出 out of memory :指程序申请内存时，没有足够的内存供申请者使用，或者说，给了你一块存储int类型数据的存储空间，但是你却存储long类型的数据，那么结果就是内存不够用，此时就会报错OOM,即所谓的内存溢出。 

内存溢出原因： 

1.内存中加载的数据量过于庞大，如一次从数据库取出过多数据； 

2.集合类中有对对象的引用，使用完后未清空，使得JVM不能回收； 

3.代码中存在死循环或循环产生过多重复的对象实体；

4.使用的第三方软件中的BUG； 

5.启动参数内存值设定的过小

内存溢出的解决方案： 

第一步，修改JVM启动参数，直接增加内存。(-Xms，-Xmx参数一定不要忘记加。)

第二步，检查错误日志，查看“OutOfMemory”错误前是否有其它异常或错误。

第三步，对代码进行走查和分析，找出可能发生内存溢出的位置。

##### GC调优

**代大小优化**

最关键参数：-Xms、 -Xmx 、-Xmn 、-XX:SurvivorRatio、-XX:MaxTenuringThreshold、-XX:PermSize、-XX:MaxPermSize

-Xms、 -Xmx 通常设置为相同的值，避免运行时要不断扩展JVM内存，这个值决定了JVM heap所能使用的最大内存。

-Xmn 决定了新生代空间的大小，新生代Eden、S0、S1三个区域的比率可以通过-XX:SurvivorRatio来控制(假如值为 4  表示：Eden:S0:S1 = 4:3:3 )

-XX:MaxTenuringThreshold 控制对象在经过多少次minor GC之后进入老年代，此参数只有在Serial 串行GC时有效。

-XX:PermSize、-XX:MaxPermSize 用来控制方法区的大小，通常设置为相同的值。

1.避免新生代大小设置过小

当新生代设置过小时，会产生两种比较明显的现象，一是minor GC次数频繁，二是可能导致 minor GC对象直接进入老年代。当老年代内存不足时，会触发Full GC。

2.避免新生代大小设置过大

新生代设置过大，会带来两个问题：一是老年代变小，可能导致Full  GC频繁执行；二是 minor GC 执行回收的时间大幅度增加。

3.避免Survivor区过大或过小

-XX:SurvivorRatio参数的值越大，就意味着Eden区域变大，minor GC次数会降低，但两块Survivor区域变小，如果超过Survivor区域内存大小的对象在minor GC后仍没被回收，则会直接进入老年代，

-XX:SurvivorRatio参数值设置过小，就意味着Eden区域变小，minor GC触发次数会增加，Survivor区域变大，意味着可以存储更多在minor GC后任存活的对象，避免其进入老年代。

4.合理设置对象在新生代存活的周期

新生代存活周期的值决定了新生代对象在经过多少次Minor GC后进入老年代。因此这个值要根据自己的应用来调优，Jvm参数上这个值对应的为-XX:MaxTenuringThreshold，默认值为15次。

**减少GC开销的措施**

　　　　1)不要显式调用System.gc()。此函数建议JVM进行主GC,虽然只是建议而非一定,但很多情况下它会触发主GC,从而增加主GC的频率,也即增加了间歇性停顿的次数。大大的影响系统性能。

　　　  2)尽量减少临时对象的使用。临时对象在跳出函数调用后,会成为垃圾,少用临时变量就相当于减少了垃圾的产生,从而延长了出现上述第二个触发条件出现的时间,减少了主GC的机会。

　　　　3)对象不用时最好显式置为Null。一般而言,为Null的对象都会被作为垃圾处理,所以将不用的对象显式地设为Null,有利于GC收集器判定垃圾,从而提高了GC的效率。

　　　　4)尽量使用StringBuffer,而不用String来累加字符串。由于String是固定长的字符串对象,累加String对象时,并非在一个String对象中扩增,而是重新创建新的String对象,如Str5=Str1+Str2+Str3+Str4,这条语句执行过程中会产生多个垃圾对象,因为对次作“+”操作时都必须创建新的String对象,但这些过渡对象对系统来说是没有实际意义的,只会增加更多的垃圾。避免这种情况可以改用StringBuffer来累加字符串,因StringBuffer是可变长的,它在原有基础上进行扩增,不会产生中间对象。

　　　　5)能用基本类型如Int,Long,就不用Integer,Long对象。基本类型变量占用的内存资源比相应对象占用的少得多,如果没有必要,最好使用基本变量。

　　　　6)尽量少用静态对象变量。静态变量属于全局变量,不会被GC回收,它们会一直占用内存。

　　　　7)分散对象创建或删除的时间。集中在短时间内大量创建新对象,特别是大对象,会导致突然需要大量内存,JVM在面临这种情况时,只能进行主GC,以回收内存或整合内存碎片,从而增加主GC的频率。集中删除对象,道理也是一样的。它使得突然出现了大量的垃圾对象,空闲空间必然减少,从而大大增加了下一次创建新对象时强制主GC的机会。

#### MQ中间件：

|            | ActiveMQ                                                | RabbitMQ                                                     | RocketMQ                                                     | Kafka                                                        | ZeroMQ               |
| ---------- | ------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------- |
| 单机吞吐量 | 比RabbitMQ低                                            | 2.6w/s（消息做持久化）                                       | 11.6w/s                                                      | 17.3w/s                                                      | 29w/s                |
| 开发语言   | Java                                                    | Erlang                                                       | Java                                                         | Scala/Java                                                   | C                    |
| 主要维护者 | Apache                                                  | Mozilla/Spring                                               | Alibaba                                                      | Apache                                                       | iMatix，创始人已去世 |
| 成熟度     | 成熟                                                    | 成熟                                                         | 开源版本不够成熟                                             | 比较成熟                                                     | 只有C、PHP等版本成熟 |
| 订阅形式   | 点对点(p2p)、广播（发布-订阅）                          | 提供了4种：direct, topic ,Headers和fanout。fanout就是广播模式 | 基于topic/messageTag以及按照消息类型、属性进行正则匹配的发布订阅模式 | 基于topic以及按照topic进行正则匹配的发布订阅模式             | 点对点(p2p)          |
| 持久化     | 支持少量堆积                                            | 支持少量堆积                                                 | 支持大量堆积                                                 | 支持大量堆积                                                 | 不支持               |
| 顺序消息   | 不支持                                                  | 不支持                                                       | 支持                                                         | 支持                                                         | 不支持               |
| 性能稳定性 | 好                                                      | 好                                                           | 一般                                                         | 较差                                                         | 很好                 |
| 集群方式   | 支持简单集群模式，比如’主-备’，对高级集群模式支持不好。 | 支持简单集群，'复制’模式，对高级集群模式支持不好。           | 常用 多对’Master-Slave’ 模式，开源版本需手动切换Slave变成Master | 天然的‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave | 不支持               |
| 管理界面   | 一般                                                    | 较好                                                         | 一般                                                         | 无                                                           | 无                   |

##### Kafka

**1.kafka又是什么？**

在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。

1）Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。

2）Kafka最初是由LinkedIn公司开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。

3）Kafka是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)称为broker。

4）无论是kafka集群，还是consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。

**2.kafka由什么组成？**

一个典型的Kafka体系架构包括若干Producer(可以是服务器日志，业务数据，页面前端产生的page view等等)，若干broker(Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高)，若干Consumer (Group)，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在consumer group发生变化时进行rebalance。Producer使用push(推)模式将消息发布到broker，Consumer使用pull(拉)模式从broker订阅并消费消息。

1）Producer ：消息生产者，就是向kafka broker发消息的客户端；

2）Consumer ：消息消费者，向kafka broker取消息的客户端；

3）Topic ：可以理解为一个队列；

4） Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic；

5）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic；

6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序；

7）Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka。

**3.kafka工作流程分析？**

写入流程：
​      1）producer先从zookeeper的 "/brokers/.../state"节点找到该partition的leader

2）producer将消息发送给该leader

3）leader将消息写入本地log

4）followers从leader pull消息，写入本地log后向leader发送ACK

5）leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK

存储策略：

无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：

1）基于时间：log.retention.hours=168

2）基于大小：log.retention.bytes=1073741824

需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。

消费过程：

能够让开发者自己控制offset，想从哪里读取就从哪里读取。

自行控制连接分区，对分区自定义进行负载均衡

对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）

消费者组：

消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。

在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。

消费方式：

consumer采用pull（拉）模式从broker中读取数据。

push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。

对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。

pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。

#### [Netty](https://thinkwon.blog.csdn.net/article/details/104391081)

Netty是 一个异步事件驱动的网络应用程序框架，用于快速开发可维护的高性能协议服务器和客户端。Netty是基于NIO的，它封装了JDK的NIO，让我们使用起来更加方法灵活。

- 高并发：Netty 是一款基于 NIO（Nonblocking IO，非阻塞IO）开发的网络通信框架，对比于 BIO（Blocking I/O，阻塞IO），他的并发性能得到了很大提高。
- 传输快：Netty 的传输依赖于零拷贝特性，尽量减少不必要的内存拷贝，实现了更高效率的传输。
- 封装好：Netty 封装了 NIO 操作的很多细节，提供了易于使用调用接口。

典型的应用有：阿里分布式服务框架 Dubbo，默认使用 Netty 作为基础通信组件，还有 RocketMQ 也是使用 Netty 作为通讯的基础。

##### Netty 高性能表现在哪些方面？

- IO 线程模型：同步非阻塞，用最少的资源做更多的事。
- 内存零拷贝：尽量减少不必要的内存拷贝，实现了更高效率的传输。
- 内存池设计：申请的内存可以重用，主要指直接内存。内部实现是用一颗二叉查找树管理内存分配情况。
- 串形化处理读写：避免使用锁带来的性能开销。
- 高性能序列化协议：支持 protobuf 等高性能序列化协议。

##### BIO、NIO和AIO的区别？

BIO：一个连接一个线程，客户端有连接请求时服务器端就需要启动一个线程进行处理。线程开销大。
伪异步IO：将请求连接放入线程池，一对多，但线程还是很宝贵的资源。

NIO：一个请求一个线程，但客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。

AIO：一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理，

BIO是面向流的，NIO是面向缓冲区的；BIO的各种流是阻塞的。而NIO是非阻塞的；BIO的Stream是单向的，而NIO的channel是双向的。

NIO的特点：事件驱动模型、单线程处理多任务、非阻塞I/O，I/O读写不再阻塞，而是返回0、基于block的传输比基于流的传输更高效、更高级的IO函数zero-copy、IO多路复用大大提高了Java网络应用的可伸缩性和实用性。基于Reactor线程模型。

在Reactor模式中，事件分发器等待某个事件或者可应用或个操作的状态发生，事件分发器就把这个事件传给事先注册的事件处理函数或者回调
